{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxpH1GkIVVmL9duNWh5ZEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarveshshirulkar/Deepfake-Detection-Model/blob/main/Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3r_nh6EY-rM",
        "outputId": "9ada4032-cfd1-41fa-c7b4-9c0deb71e396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the sympy/torch conflict\n",
        "!pip install --upgrade sympy==1.12\n",
        "!pip install --force-reinstall torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8-f7wJ-VkG3K",
        "outputId": "b2e4ad59-ad93-4bcd-f04a-7c2e9ea0e499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.12\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "d6d631737b2a43e29bd69ff5d2f14c45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from transformers import ViTModel, ViTConfig\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    # Model\n",
        "    image_size = 224\n",
        "    patch_size = 16\n",
        "    num_classes = 2\n",
        "\n",
        "    # Training\n",
        "    frames_per_video = 10  # Was 3 → More temporal info\n",
        "    batch_size = 8        # Was 16 → Better gradient updates\n",
        "    epochs = 20           # Was 5 → Needs more training\n",
        "    lr = 0.0001\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Video Processing\n",
        "    frames_per_video = 3  # Reduced for short videos\n",
        "\n",
        "# 1. Video Processing Functions\n",
        "def extract_frames(video_path, num_frames):\n",
        "    \"\"\"Extract uniformly spaced frames from video\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        frame_idx = i * (total_frames // num_frames)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(Image.fromarray(frame))\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# 2. Dataset Class\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, real_dir, fake_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # Process real videos\n",
        "        for vid in os.listdir(real_dir):\n",
        "            frames = extract_frames(os.path.join(real_dir, vid), Config.frames_per_video)\n",
        "            self.samples.extend([(frame, 0) for frame in frames])\n",
        "\n",
        "        # Process fake videos\n",
        "        for vid in os.listdir(fake_dir):\n",
        "            frames = extract_frames(os.path.join(fake_dir, vid), Config.frames_per_video)\n",
        "            self.samples.extend([(frame, 1) for frame in frames])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.samples[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# 3. Vision Transformer Model\n",
        "class DeepfakeViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        config = ViTConfig(\n",
        "            image_size=Config.image_size,\n",
        "            patch_size=Config.patch_size,\n",
        "            num_classes=Config.num_classes\n",
        "        )\n",
        "        self.vit = ViTModel(config)\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(768, 256),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(0.2),\n",
        "        #     nn.Linear(256, 2)\n",
        "        # )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 512),  # Deeper\n",
        "            nn.GELU(),            # Better than ReLU\n",
        "            nn.Dropout(0.3),      # More regularization\n",
        "            nn.Linear(512, 2)\n",
        ")\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.vit(pixel_values=x)\n",
        "        cls_token = outputs.last_hidden_state[:, 0]\n",
        "        return self.classifier(cls_token)\n",
        "\n",
        "# 4. Training Function\n",
        "def train_model(real_dir, fake_dir):\n",
        "    # Data preparation\n",
        "    # transform = transforms.Compose([\n",
        "    #     transforms.Resize((Config.image_size, Config.image_size)),\n",
        "    #     transforms.ToTensor(),\n",
        "    #     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    # ])\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((Config.image_size, Config.image_size)),\n",
        "    transforms.RandomHorizontalFlip(),  # New\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),  # New\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "])\n",
        "\n",
        "    dataset = VideoDataset(real_dir, fake_dir, transform)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size)\n",
        "\n",
        "    # Model setup\n",
        "    model = DeepfakeViT().to(Config.device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(Config.epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        f1 = f1_score(all_labels, all_preds)\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "        tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Results:\")\n",
        "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "        print(f\"Confusion Matrix:\\nTP: {tp} | FP: {fp}\\nFN: {fn} | TN: {tn}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Test Function\n",
        "def test_video(model, video_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((Config.image_size, Config.image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    frames = extract_frames(video_path, Config.frames_per_video)\n",
        "    model.eval()\n",
        "    fake_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frame in frames:\n",
        "            img_tensor = transform(frame).unsqueeze(0).to(Config.device)\n",
        "            output = model(img_tensor)\n",
        "            prob = torch.softmax(output, dim=1)[0, 1].item()\n",
        "            fake_probs.append(prob)\n",
        "\n",
        "    avg_prob = np.mean(fake_probs)\n",
        "    prediction = \"FAKE\" if avg_prob > 0.5 else \"REAL\"\n",
        "\n",
        "    print(f\"\\nTest Results for {video_path}:\")\n",
        "    print(f\"Prediction: {prediction} (Confidence: {avg_prob:.4f})\")\n",
        "    print(f\"Frame-level probabilities: {[round(p, 4) for p in fake_probs]}\")\n",
        "    return prediction, avg_prob\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Train the model (update paths)\n",
        "    print(\"Starting training...\")\n",
        "    model = train_model(\n",
        "        real_dir=\"/content/drive/MyDrive/Documents/SDFVD/SDFVD/videos_real\",\n",
        "        fake_dir=\"/content/drive/MyDrive/Documents/SDFVD/SDFVD/videos_fake\"\n",
        "    )\n",
        "    torch.save(model.state_dict(), \"deepfake_vit.pth\")\n",
        "\n",
        "    # 2. Test a video\n",
        "    print(\"\\nTesting sample video...\")\n",
        "    test_model = DeepfakeViT().to(Config.device)\n",
        "    test_model.load_state_dict(torch.load(\"deepfake_vit.pth\"))\n",
        "    test_video(test_model, \"/content/dfvideo.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rou0FBzmYYRH",
        "outputId": "545ba099-8471-4b90-d4cf-2d8956e7683e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 32/32 [00:12<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Results:\n",
            "Train Loss: 0.7794\n",
            "Val Loss: 0.7106\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 32/32 [00:11<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Results:\n",
            "Train Loss: 0.7157\n",
            "Val Loss: 0.7034\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 32/32 [00:11<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Results:\n",
            "Train Loss: 0.7383\n",
            "Val Loss: 0.7018\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 32/32 [00:10<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Results:\n",
            "Train Loss: 0.7112\n",
            "Val Loss: 0.6948\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 32/32 [00:10<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Results:\n",
            "Train Loss: 0.7096\n",
            "Val Loss: 0.6945\n",
            "Accuracy: 0.4531 | F1: 0.0541 | AUC: 0.4668\n",
            "Confusion Matrix:\n",
            "TP: 1 | FP: 3\n",
            "FN: 32 | TN: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 32/32 [00:10<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Results:\n",
            "Train Loss: 0.7009\n",
            "Val Loss: 0.6964\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 32/32 [00:10<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Results:\n",
            "Train Loss: 0.7001\n",
            "Val Loss: 0.6938\n",
            "Accuracy: 0.5156 | F1: 0.6804 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 33 | FP: 31\n",
            "FN: 0 | TN: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 32/32 [00:10<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Results:\n",
            "Train Loss: 0.7058\n",
            "Val Loss: 0.7122\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 32/32 [00:10<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 Results:\n",
            "Train Loss: 0.6992\n",
            "Val Loss: 0.6975\n",
            "Accuracy: 0.5156 | F1: 0.6804 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 33 | FP: 31\n",
            "FN: 0 | TN: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 32/32 [00:10<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 Results:\n",
            "Train Loss: 0.7002\n",
            "Val Loss: 0.7017\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 32/32 [00:10<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 Results:\n",
            "Train Loss: 0.7138\n",
            "Val Loss: 0.6964\n",
            "Accuracy: 0.4531 | F1: 0.0000 | AUC: 0.4677\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 2\n",
            "FN: 33 | TN: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 32/32 [00:10<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 Results:\n",
            "Train Loss: 0.6992\n",
            "Val Loss: 0.6961\n",
            "Accuracy: 0.4375 | F1: 0.5814 | AUC: 0.4272\n",
            "Confusion Matrix:\n",
            "TP: 25 | FP: 28\n",
            "FN: 8 | TN: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 32/32 [00:10<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 Results:\n",
            "Train Loss: 0.7008\n",
            "Val Loss: 0.7023\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 32/32 [00:10<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 Results:\n",
            "Train Loss: 0.7191\n",
            "Val Loss: 0.6967\n",
            "Accuracy: 0.4688 | F1: 0.0000 | AUC: 0.4839\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 1\n",
            "FN: 33 | TN: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 32/32 [00:10<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 Results:\n",
            "Train Loss: 0.6988\n",
            "Val Loss: 0.6976\n",
            "Accuracy: 0.5156 | F1: 0.6804 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 33 | FP: 31\n",
            "FN: 0 | TN: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 32/32 [00:10<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 Results:\n",
            "Train Loss: 0.6977\n",
            "Val Loss: 0.6962\n",
            "Accuracy: 0.4688 | F1: 0.0556 | AUC: 0.4829\n",
            "Confusion Matrix:\n",
            "TP: 1 | FP: 2\n",
            "FN: 32 | TN: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 32/32 [00:10<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 Results:\n",
            "Train Loss: 0.7112\n",
            "Val Loss: 0.6955\n",
            "Accuracy: 0.5156 | F1: 0.6804 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 33 | FP: 31\n",
            "FN: 0 | TN: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 32/32 [00:10<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 Results:\n",
            "Train Loss: 0.6952\n",
            "Val Loss: 0.6958\n",
            "Accuracy: 0.5156 | F1: 0.6804 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 33 | FP: 31\n",
            "FN: 0 | TN: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 32/32 [00:10<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 Results:\n",
            "Train Loss: 0.6974\n",
            "Val Loss: 0.7116\n",
            "Accuracy: 0.4688 | F1: 0.0556 | AUC: 0.4829\n",
            "Confusion Matrix:\n",
            "TP: 1 | FP: 2\n",
            "FN: 32 | TN: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 32/32 [00:10<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 Results:\n",
            "Train Loss: 0.7035\n",
            "Val Loss: 0.7012\n",
            "Accuracy: 0.4844 | F1: 0.0000 | AUC: 0.5000\n",
            "Confusion Matrix:\n",
            "TP: 0 | FP: 0\n",
            "FN: 33 | TN: 31\n",
            "\n",
            "Testing sample video...\n",
            "\n",
            "Test Results for /content/dfvideo.mp4:\n",
            "Prediction: REAL (Confidence: 0.4669)\n",
            "Frame-level probabilities: [0.4671, 0.4666, 0.4669]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "             Input Token Embeddings (197 x D)\n",
        "                          ↓\n",
        "          ┌───────────────────────────────────┐\n",
        "          │  Multi-Head Self-Attention (MHSA) │\n",
        "          └───────────────────────────────────┘\n",
        "                          ↓\n",
        "             Add & LayerNorm (Residual #1)\n",
        "                          ↓\n",
        "     ┌─────────────────────────────────────────┐\n",
        "     │   Feedforward Network (MLP / FFN block) │\n",
        "     └─────────────────────────────────────────┘\n",
        "                          ↓\n",
        "             Add & LayerNorm (Residual #2)\n",
        "                          ↓\n",
        "                  Output Token Embeddings\n"
      ],
      "metadata": {
        "id": "ZJrxHg846Ndl"
      }
    }
  ]
}